[
{"ID": 1274058771320275035, "Timestamp": "2024-08-16 17:34:41", "Contents": "the issue was in the generate_until function of MyLM -- i had originally set it to generate results for each request isntead of just combining all the requests into one list and generating it at once", "Attachments": ""},
{"ID": 1273786460935295068, "Timestamp": "2024-08-15 23:32:38", "Contents": "i was able to fix it, thank you!", "Attachments": ""},
{"ID": 1273773802236411904, "Timestamp": "2024-08-15 22:42:20", "Contents": "hi hailey sorry to bother again, but soemthing that i noticed while running the code is that our dataset contains 200 problems, and the current eval code is processing each quesiton individually (as shown in the image) as opposed to looking at 200 at a time. this is making the process rather slow. is this an expected outcome or is there soemthing we can change in our code to avoid this? thank you so much, and once again i really apprecaite all the help", "Attachments": "https://cdn.discordapp.com/attachments/1272759995561152562/1273773801838215219/Screenshot_2024-08-15_at_3.41.21_PM.png?ex=0&is=6703122a&hm=c073a1b80821f1d9bf414b44827cc298a9ee9abc40d399c1c23fbdb6488529ce&uc=dp&"},
{"ID": 1273756382847434789, "Timestamp": "2024-08-15 21:33:06", "Contents": "Hi hailey, ive been able to get the code to work now, thank you so much for your help, i really appreciate it!", "Attachments": ""},
{"ID": 1273729042708959232, "Timestamp": "2024-08-15 19:44:28", "Contents": "ill look into the examples and play around with the code a bit more, thank you so much!", "Attachments": ""},
{"ID": 1273381282201473025, "Timestamp": "2024-08-14 20:42:35", "Contents": "we're getting errors with the process_docs function (we're processing results with process_results: !function utils.process_docs in our .yaml file). what exactly does the rest of the eval code expect for the output of this function? Should we be using this function to compare the output of our model with the expected output? Thank you!", "Attachments": ""},
{"ID": 1273029921790230640, "Timestamp": "2024-08-13 21:26:25", "Contents": "Hi Hailey, thank you so much for offering to help, we really apprecaite it. Though the code to our database is currently private, we can share it with your github it that would be easiest for you.\n\nThe current format of the data is one json file, each element of the dataset has the following properties:\nid, original problem, original solution, problem, solution, source, type (we want to be using the problem/solution and not the originals). \n\nSo far we've been trying to follow the publically avaible instructions on how to create a new task and run lm_eval.simple_evaluate. We've made a yaml file and utils file under the tasks folder (where other tasks such as the MATH dataset is stored), but we have been receiving errors. Is directly creating a new task under our cloned lm-eval folder the best approach?", "Attachments": ""}
]